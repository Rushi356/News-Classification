# -*- coding: utf-8 -*-
"""News_Word2Vec_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1umDeDTfxwIP6Ihrk-EtOi1_MUxeP-UEC
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import gensim
from gensim.models import Word2Vec
import re
import nltk
from nltk.corpus import stopwords

"""## Importing Data"""

df_train = pd.read_excel('Data_Train.xlsx')
df_test = pd.read_excel('Data_Test.xlsx')

df_train.head(2)

df_test.head(2)

"""## Exploratory Data Analysis"""

df_train.SECTION.value_counts().plot(kind='bar')

politics = df_train[df_train['SECTION'] == 0]
technology = df_train[df_train['SECTION'] == 1]
entertainment = df_train[df_train['SECTION'] == 2]
business = df_train[df_train['SECTION'] == 3]
count = [politics['SECTION'].count(),technology['SECTION'].count(),entertainment['SECTION'].count(),business['SECTION'].count()]
plt.pie(count,labels=['politics','technology','entertainment','business'],
       explode = (0.05, 0.05, 0.05, 0.05),shadow = True,autopct = "%1.1f%%")

df_train.shape

df_train.info()

total_len = len(df_train)
politics_len = len(df_train[df_train.SECTION == 0])
tech_len = len(df_train[df_train.SECTION == 1])
enter_len = len(df_train[df_train.SECTION == 2])
buss_len = len(df_train[df_train.SECTION == 3])

pol = 100*(politics_len/total_len)
tech = 100*(tech_len/total_len)
enter = 100*(enter_len/total_len)
buss = 100*(buss_len/total_len)
print(f'percentage of politics news: {pol}')
print(f'percentage of technoglogy news: {tech}')
print(f'percentage of entertenment news: {enter}')
print(f'percentage of bussiness news: {buss}')

"""## Cleaning the text

1. Removing the special characters
2. Lowering the text.
3. Lemmatization
4. Removing the stop words
"""

nltk.download('stopwords')
nltk.download('wordnet')

import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
wl = WordNetLemmatizer()

def text_preprocessing(text):
    text = re.sub('[^a-zA-Z]',' ',text)
    text = text.lower()
    text = text.split()
    text = [wl.lemmatize(word) for word in text if not word in stopwords.words('english')]
    text = ' '.join(text)
    return text

df_train['STORY'] = df_train['STORY'].apply(text_preprocessing)
df_test['STORY'] = df_test['STORY'].apply(text_preprocessing)

from nltk.tokenize import word_tokenize
nltk.download('punkt')

"""## Creating Word Cloud for Differant Section of News

### Politics
"""

from wordcloud import WordCloud
politics_text = ' '.join(cat.split()[1] for cat in politics['STORY'])
politics_cloud = WordCloud(collocations = False, background_color = 'white').generate(politics_text)
plt.imshow(politics_cloud,interpolation='bilinear')
plt.axis('off')
plt.show()

"""### Technology"""

#from wordcloud import WordCloud
technology_text = ' '.join(cat.split()[1] for cat in technology['STORY'])
technology_cloud = WordCloud(collocations = False, background_color = 'white').generate(technology_text)
plt.imshow(technology_cloud,interpolation='bilinear')
plt.axis('off')
plt.show()

"""### Entertainment"""

#from wordcloud import WordCloud
entertainment_text = ' '.join(cat.split()[1] for cat in entertainment['STORY'])
entertainment_cloud = WordCloud(collocations = False, background_color = 'white').generate(entertainment_text)
plt.imshow(entertainment_cloud,interpolation='bilinear')
plt.axis('off')
plt.show()

"""### Business"""

#from wordcloud import WordCloud
business_text = ' '.join(cat.split()[1] for cat in business['STORY'])
business_cloud = WordCloud(collocations = False, background_color = 'white').generate(business_text)
plt.imshow(business_cloud,interpolation='bilinear')
plt.axis('off')
plt.show()

# Training data
lst_story = []
for line in df_train['STORY']:
  tokens = word_tokenize(line)
  lst_story.append(tokens)

# Testing data
lst_story_test = []
for line in df_test['STORY']:
  tokens = word_tokenize(line)
  lst_story_test.append(tokens)

"""## Training Word2vec Model using Training data"""

EMBEDDING_DIM = 100
# train word2vec model
model_wv = gensim.models.Word2Vec(sentences=lst_story,size=EMBEDDING_DIM,window=5,workers=4,min_count=1)

# vocab size
words = list(model_wv.wv.vocab)
print(f'vocabulory size: {len(words)}')

# Save model in ASCII format
filename = 'news_cat_w2v.txt'
model_wv.wv.save_word2vec_format(filename,binary=False)

model_wv.wv.most_similar('king')

#Extracting the word embedding from stores file.
import os
embedding_index = {}
f = open(os.path.join('','news_cat_w2v.txt'),encoding='utf-8')
for line in f:
  values = line.split()
  word = values[0]
  coefs = np.asarray(values[1:])
  embedding_index[word] = coefs
f.close()

max_len = max([len(s) for s in lst_story])
max_len

# vectorize the text samples into 2D integer tensor
from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences

VALIDATION_SPLIT = 0.2

tokenizer_obj = Tokenizer()
tokenizer_obj.fit_on_texts(lst_story)
sequences = tokenizer_obj.texts_to_sequences(lst_story)

# pad sequences
word_index = tokenizer_obj.word_index
story_pad = pad_sequences(sequences,maxlen=max_len)
section = df_train['SECTION'].values
print('shape of story tensor:',story_pad.shape)
print('shape of section tensor:',section.shape)

len(word_index)

# spliting data into training and validation set
indices = np.arange(story_pad.shape[0])
np.random.shuffle(indices)
story_pad = story_pad[indices]
section = section[indices]
num_validation_sample = int(VALIDATION_SPLIT*story_pad.shape[0])
X_train = story_pad[:-num_validation_sample]
X_test = story_pad[-num_validation_sample:]
y_train = section[:-num_validation_sample]
y_test = section[-num_validation_sample:]

from keras.utils import np_utils

y_train_en = np_utils.to_categorical(y_train)
y_test_en = np_utils.to_categorical(y_test)

print('shape of X_train tensor',X_train.shape)
print('shape of X_test tensor',X_test.shape)
print('shape of y_train tensor',y_train_en.shape)
print('shape of y_test tensor',y_test_en.shape)

type(word_index)

EMBEDDING_DIM = 100
num_words = len(word_index) + 1
embedding_matrix = np.zeros((num_words,EMBEDDING_DIM))

for word, i in word_index.items():
  if i > num_words:
    continue
  embedding_vector = embedding_index.get(word)
  if embedding_vector is not None:
    embedding_matrix[i] = embedding_vector

embedding_matrix.shape

from keras.models import Sequential
from keras.layers import Dense, Embedding, Flatten, LSTM
from keras.initializers import constant
from keras.layers.embeddings import Embedding

"""### Creating LSTM Model"""

# Define Model
model_nn = Sequential()

embedding_layer = Embedding(num_words,EMBEDDING_DIM,
                            embeddings_initializer=constant(embedding_matrix),
                            input_length=max_len,
                            trainable=False)

model_nn.add(embedding_layer)
model_nn.add(LSTM(128,dropout=0.2,recurrent_dropout=0.2))
model_nn.add(Dense(4,activation='softmax'))

model_nn.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model_nn.summary())

import tensorflow as tf
tf.keras.utils.plot_model(model_nn)

clf_model = model_nn.fit(X_train,y_train_en,batch_size=128,epochs=5,validation_split=0.2,verbose=2)

loss, accuracy = model_nn.evaluate(X_test,y_test_en,batch_size=128)

y_pred = model_nn.predict(X_test)

X_test[1].shape

pred = model_nn.predict(X_test[0:1])
pred = pred.round()
pred

from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(accuracy_score(y_test_en,y_pred.round()))
print(classification_report(y_test_en,y_pred.round()))

"""### Plot the accuracy and loss over time"""

history_dict = clf_model.history
history_dict

history_dict = clf_model.history
print(history_dict.keys())

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)
fig = plt.figure(figsize=(10,6))
fig.tight_layout(pad=2.0)

plt.subplot(2, 1, 1)

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(2, 1, 2)
plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuarcy')
plt.title('Trainig and Validation accuarcy')
plt.xlabel('Epochs')
plt.ylabel('Accuarcy')
plt.legend(loc='lower right')

## Tokenizing and padding the test data
tokenizer_obj.fit_on_texts(lst_story_test)
sequences_test = tokenizer_obj.texts_to_sequences(lst_story_test)
story_pad_test = pad_sequences(sequences_test,maxlen=max_len)
print('shape of test story',story_pad_test.shape)

y_pred_test_data = model_nn.predict(story_pad_test)

import pickle
filename = 'news.pkl'
pickle.dump(model_nn,open(filename,'wb'))

model_nn.save('news_cat.h5')

